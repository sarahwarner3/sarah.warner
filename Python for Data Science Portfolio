------ 911 CALLS PROJECT ------ 

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

df = pd.read_csv('911.csv')

#Find top 5 zipcodes for 911 calls:
  df['zip'].value_counts().head(5)

#Find top 5 townships for 911 calls:
  df['twp'].value_counts().head(5)

#Create new column for call reason:
  df['Reason'] = df['title'].apply(lambda x: x.split(':')[0])

#Find most common reason for a 911 call:
  df['Reason'].value_counts()
  sns.countplot(data = df, x = 'Reason', palette = 'Set2')

#Convert timeStamp column from str to DateTime objects:
  df['timeStamp'] = pd.to_datetime(df['timeStamp'])

#Create Hour, Month, and Day of Week columns for each 911 call:
  time = df['timeStamp'].iloc[0]
  time.hour
  df['Hour'] = df['timeStamp'].apply(lambda time: time.hour)
  df['Month'] = df['timeStamp'].apply(lambda time: time.month)
  df['Day of Week'] = df['timeStamp'].apply(lambda time: time.dayofweek)
  dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}
  df['Day of Week'] = df['Day of Week'].map(dmap)

#Create a countplot for Day of Week with Reason:
  sns.countplot(data = df, x = 'Day of Week', hue = 'Reason')
  plt.legend(loc='center left', bbox_to_anchor=(1, .5))

#Create a countplot for Month with Reason:
  sns.countplot(data = df, x = 'Month', hue = 'Reason', palette = 'Set1')
  plt.legend(loc='center left', bbox_to_anchor=(1, .5))

#Create a groupby object for the Month and aggregate the data to fill in missing months:
  byMonth = df.groupby('Month').count()
  byMonth.head()

#Create a plot showing calls per month:
  byMonth['lat'].plot()
  sns.lmplot(data = byMonth.reset_index(), x = 'Month', y = 'lat')

#Create a Date column from the TimeStamp column:
  df['Date'] = df['timeStamp'].apply(lambda t: t.date())

#Create a plot of 911 calls by Date and Reason:
  df[df['Reason'] == 'Traffic'].groupby('Date').count()['lat'].plot()
  plt.tight_layout()
  plt.title('Traffic')

  df[df['Reason'] == 'Fire'].groupby('Date').count()['lat'].plot()
  plt.tight_layout()
  plt.title('Fire')

  df[df['Reason'] == 'EMS'].groupby('Date').count()['lat'].plot()
  plt.tight_layout()
  plt.title('EMS')

#Create a heatmap of calls by Hour and Day of Week:
  dayHour = df.groupby(by = ['Day of Week','Hour']).count()['Reason'].unstack()
  plt.figure(figsize = (12,6))
  sns.heatmap(data = dayHour, cmap = 'viridis')

#Create a heatmap of calls by Month and Day of Week:
  dayMonth = df.groupby(by = ['Day of Week','Month']).count()['Reason'].unstack()
  plt.figure(figsize = (12,6))
  sns.heatmap(data = dayMonth, cmap = 'coolwarm')




------ FINANCE PROJECT ------ 

from pandas_datareader import data, wb
import yfinance as yf
import pandas as pd
import numpy as np
import seaborn as sns
import datetime
import matplotlib.pyplot as plt
import plotly
import cufflinks as cf
cf.go_offline()

all_banks = pd.read_pickle('all_banks.pkl')

start = datetime.datetime(2006,1,1)
end = datetime.datetime(2016,1,1)

#Create dataframes for the banks
  BAC = yf.download('BAC', start, end) #Bank of America
  C = yf.download('C', start, end) #CitiGroup
  GS = yf.download('GS', start, end) # Goldman Sachs
  JPM = yf.download('JPM', start, end) #JPMorgan Chase
  MS = yf.download('MS', start, end) # Morgan Stanley
  WFC = yf.download('WFC', start, end) #Wells Fargo

#Concatenate dataframes into a single dataframe
  tickers = ['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC']
  bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC], axis = 1, keys = tickers)

#Find max close price for each bank's stock:
  all_banks.xs(key = 'Close', axis = 1, level = 'Stock Info').max()

#Add a column of return values for each bank stock at close:
  returns = pd.DataFrame()
  for tick in tickers:
    returns[tick+' Return'] = all_banks[tick]['Close'].pct_change()

#Create a plot showing close price for each bank over time
  for tick in tickers:
    all_banks[tick]['Close'].plot(figsize=(12,4),label=tick)
  plt.legend()

#Create a plot of the rolling 30 day average against the Close Price for Bank Of America's stock for the year 2008
  plt.figure(figsize=(12,6))
  BAC['Close'].loc['2008-01-01':'2009-01-01'].rolling(window=30).mean().plot(label = '30 Day Avg')
  BAC['Close'].loc['2008-01-01':'2009-01-01'].plot(label = 'BAC CLOSE')
  plt.legend()

#Create a candle plot of Bank of America's stock from Jan 1st 2015 to Jan 1st 2016
  close_corr = all_banks.xs(key='Close',axis=1,level='Stock Info').corr()
  close_corr.plot(kind='kde')




------ LINEAR REGRESSION PROJECT ------ 

#Congratulations! You just got some contract work with an Ecommerce company based in New York City that sells clothing online but they also have in-store style and clothing advice sessions.
#Customers come in to the store, have sessions/meetings with a personal stylist, then they can go home and order either on a mobile app or website for the clothes they want.
#The company is trying to decide whether to focus their efforts on their mobile app experience or their website. They've hired you on contract to help them figure it out!

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

customers = pd.read_csv('Ecommerce Customers')

#Explore relationships across the entire data set
  sns.pairplot(data = customers)
  plt.show()

#Create a linear model plot of Yearly Amount Spent vs. Length of Membership
  sns.lmplot(data=customers, x = 'Length of Membership', y = 'Yearly Amount Spent')
  plt.grid()
  plt.show()

#Train and test the data
  customers.columns
  X = customers[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']]
  y = customers['Yearly Amount Spent']
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)

#Train the model
  from sklearn.linear_model import LinearRegression
  lm = LinearRegression()
  lm.fit(X_train, y_train)
  print('Coefficients: \n', lm.coef_)
  cdf = pd.DataFrame(lm.coef_, X.columns, columns = ['Coeff'])
  cdf

#Predict test data
  predictions = lm.predict(X_test)
  plt.scatter(y_test, predictions)
  plt.xlabel('Y Test')
  plt.ylabel('Predicted Y')
  plt.show()

#Evaluate the model
  from sklearn import metrics
  metrics.mean_absolute_error(y_test, predictions)
  metrics.mean_squared_error(y_test, predictions)
  np.sqrt(metrics.mean_squared_error(y_test, predictions))

#Residuals
  sns.displot((y_test - predictions), kde = True, bins = 40)
  plt.show()

#Conclusion
 cdf
   #For every unit increase in session length, Yearly amount spent increases by $25.98
   #For every unit increase in time on app, Yearly amount spent increases by $38.59
   #For every unit increase in time on website, Yearly amount spent increases by $0.19
   #For every unit increase in length of membership, Yearly amount spent increases by $61.28
#The company should focus more on their mobile app.




------ LOGISTIC REGRESSION PROJECT ------ 

#Work with an advertising data set and indicate whether or not a particular internet user clicked on an Advertisement.
#Try to create a model that will predict whether or not they will click on an ad based off the features of that user.

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline

ad_data = pd.read_csv('advertising.csv')

#Explore the data
  sns.jointplot(data = ad_data, x = 'Age', y = 'Area Income')
  plt.show()
  sns.jointplot(data = ad_data, x = 'Daily Time Spent on Site', y = 'Daily Internet Usage')
  plt.show()
  sns.pairplot(data = ad_data, hue = 'Clicked on Ad')
  plt.show()

#Train and test the data
  from sklearn.model_selection import train_test_split
  ad_data.columns
  X = ad_data[['Age', 'Area Income', 'Daily Internet Usage', 'Male', 'Daily Time Spent on Site']]
  y = ad_data['Clicked on Ad']
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)
  from sklearn.linear_model import LogisticRegression
  logmodel = LogisticRegression()
  logmodel.fit(X_train, y_train)

#Predictions and Evaluations
  predictions = logmodel.predict(X_test)
  from sklearn.metrics import classification_report, confusion_matrix
  print(classification_report(y_test, predictions))
  print(confusion_matrix(y_test, predictions))




------ K NEAREST NEIGHBORS PROJECT ------ 

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn

df = pd.read_csv('KNN_Project_Data')

#Explore the data
  sns.pairplot(data = df, hue = 'TARGET CLASS')

#Standardize the variables
  from sklearn.preprocessing import StandardScaler
  scaler = StandardScaler()
  scaler.fit(df.drop('TARGET CLASS', axis = 1))
  scaled_features = scaler.transform(df.drop('TARGET CLASS', axis = 1))
  scaled_features
  df_features = pd.DataFrame(scaled_features, columns = df.columns[:-1])
  df_features

# Train, test, split
  from sklearn.model_selection import train_test_split
  X = df_features
  y = df['TARGET CLASS']
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)

#Use KNN
  from sklearn.neighbors import KNeighborsClassifier
  knn = KNeighborsClassifier(n_neighbors = 1)
  knn.fit(X_train, y_train)

#Predictions and evaluations
  prediction = knn.predict(X_test)
  from sklearn.metrics import confusion_matrix, classification_report
  print(confusion_matrix(y_test, prediction))
  print(classification_report(y_test, prediction))

#Choose a K value
  error_rate = []

  for i in range (1,60):
     knn = KNeighborsClassifier(n_neighbors = i)
     knn.fit(X_train, y_train)
     prediction_i = knn.predict(X_test)
     error_rate.append(np.mean(prediction_i != y_test))

  plt.figure(figsize = (12, 8))
  plt.plot(range (1,60),error_rate, color = 'green', linestyle = '--', marker = 'o', markerfacecolor = 'black', markersize = 10)
  plt.title('Error Rate vs K Value')
  plt.xlabel('K Value')
  plt.ylabel('Error Rate')

#Retrain with new K value
  knn = KNeighborsClassifier(n_neighbors = 30)
  knn.fit(X_train, y_train)
  new_prediciton = knn.predict(X_test)

  print(confusion_matrix(y_test, new_prediciton))
  print('\n')
  print(classification_report(y_test, new_prediciton))




------ RANDOM FOREST PROJECT ------ 

#Explore publicly available data from LendingClub.com. Lending Club connects people who need money (borrowers) with people who have money (investors).
#Hopefully, as an investor you would want to invest in people who showed a profile of having a high probability of paying you back. Try to create a model that will help predict this.
#Use lending data from 2007-2010 and to classify and predict whether or not the borrower paid back their loan in full.

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('loan_data.csv')

#Explore the data
  plt.figure(figsize = (10,6))
  sns.set_style('darkgrid')
  data[data['credit.policy']==1]['fico'].hist(bins = 30, alpha = 0.5, color = 'blue', label = 'Credit Policy = 1')
  data[data['credit.policy']==0]['fico'].hist(bins = 30, alpha = 0.5, color = 'red', label = 'Credit Policy = 0')
  plt.legend()
  plt.xlabel('FICO')

  plt.figure(figsize = (10,6))
  sns.set_style('darkgrid')
  data[data['not.fully.paid']==1]['fico'].hist(bins = 30, alpha = 0.5, color = 'blue', label = 'Not Fully Paid = 1')
  data[data['not.fully.paid']==0]['fico'].hist(bins = 30, alpha = 0.5, color = 'red', label = 'Not Fully Paid = 0')
  plt.legend()
  plt.xlabel('FICO')

  sns.jointplot(data = data, x = 'fico', y = 'int.rate', color = 'purple')

  plt.figure(figsize = (10,4))
  sns.lmplot(data = data, x = 'fico', y = 'int.rate', hue = 'credit.policy', col = 'not.fully.paid', palette = 'Set1')

#Set up the data
  data.info()
  cat_feats = ['purpose']
  final_data = pd.get_dummies(data, columns = cat_feats, drop_first = True)
  final_data

#Train, test, split
  from sklearn.model_selection import train_test_split
  X = final_data.drop('not.fully.paid', axis = 1)
  y = final_data['not.fully.paid']
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)

#Train decision tree model
  from sklearn.tree import DecisionTreeClassifier
  dtree = DecisionTreeClassifier()
  dtree.fit(X_train, y_train)

#Predictions and evaluation of the decision tree
  from sklearn.metrics import classification_report, confusion_matrix
  predictions = dtree.predict(X_test)
  print(classification_report(y_test, predictions))
  print(confusion_matrix(y_test, predictions))

#Train the random forest model
  from sklearn.ensemble import RandomForestClassifier
  rfc = RandomForestClassifier()
  rfc.fit(X_train, y_train)

#Predictions and evaluation
  new_predictions = rfc.predict(X_test)
  print(classification_report(y_test, new_predictions))
  print(confusion_matrix(y_test, new_predictions))




------ NATURAL LANGUAGE PROCESSING PROJECT ------ 

#Attempt to classify Yelp Reviews into 1 star or 5 star categories based off the text content in the reviews.

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

yelp = pd.read_csv('yelp.csv')

yelp['text length'] = yelp['text'].apply(len)

#Explore the data
  p1 = sns.FacetGrid(yelp, col = "stars")
  p1.map(sns.histplot, "text length", bins=20)

  sns.boxplot(data = yelp, x = 'stars', y = 'text length', hue = 'stars', legend = False)

  sns.countplot(data = yelp, x = 'stars', hue = 'stars', legend = False)

  stars = yelp.groupby('stars').mean(numeric_only = True)
  stars

  stars_corr = stars.corr()
  stars_corr
  sns.heatmap(data = stars_corr, cmap = 'coolwarm', annot = True)

#NLP classification
  yelp_class = yelp[(['stars'] == 1) | (yelp['stars'] == 5)]
  yelp_class.head()
  X = yelp_class['text']
  y = yelp_class['stars']
  from sklearn.feature_extraction.text import CountVectorizer
  cv = CountVectorizer()
  X = cv.fit_transform(X)

#Train, test, split
  from sklearn.model_selection import train_test_split
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)

#Train the model
  from sklearn.naive_bayes import MultinomialNB
  nb = MultinomialNB()
  nb.fit(X_train, y_train)

#Predictions and evaluations
  predictions = nb.predict(X_test)
  from sklearn.metrics import confusion_matrix, classification_report
  print(confusion_matrix(y_test, predictions))
  print('\n')
  print(classification_report(y_test, predictions))

#Using text processing
  from sklearn.feature_extraction.text import TfidfTransformer
  from sklearn.pipeline import Pipeline
  pipeline = Pipeline([
     ('bow', CountVectorizer()), #strings to token integer counts
     ('tfidf', TfidfTransformer()), #integer counts to weighted TF-IDF scores
     ('classifier', MultinomialNB()), #train on TF-IDF vectors with Naive Bayes classifier])

#Using the pipeline
  X = yelp_class['text']
  y = yelp_class['stars']
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)
  pipeline.fit(X_train, y_train)
  predictions = pipeline.predict(X_test)
  print(confusion_matrix(y_test, predictions))
  print('\n')
  print(classification_report(y_test, predictions))
  



------ NEURAL NETWORKS PROJECT ------ 

#Use historical data from LendingClub on loans given out with information on whether the borrower defaulted, build a model that can predict if a borrower will pay back their loan.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('TensorFlow_FILES/lending_club_loan_two.csv')

#Explore the data
  sns.countplot(data = df, x = 'loan_status', hue = 'loan_status', palette = 'Set2')
  sns.histplot(data = df, x = 'loan_amnt', bins=40)
  sns.heatmap(df.corr(numeric_only=True),annot=True, cmap='viridis')
  sns.scatterplot(data = df, x = 'installment', y = 'loan_amnt')

  grade_order = sorted(df['grade'].unique())
  sns.countplot(data = df, x = 'grade', hue = 'loan_status', order = grade_order, palette = 'Paired')

  subgrade_order = sorted(df['sub_grade'].unique())
  sns.countplot(data = df, x = 'sub_grade', hue = 'sub_grade', palette = 'viridis', order = subgrade_order, hue_order = subgrade_order)
  sns.countplot(data = df, x = 'sub_grade', hue = 'loan_status', palette = 'viridis', order = subgrade_order)

  f_and_g = df[(df['grade'] == 'G') | (df['grade'] == 'F')]
  subgrade_order = sorted(f_and_g['sub_grade'].unique())
  sns.countplot(data = f_and_g, x = 'sub_grade', hue = 'sub_grade', palette = 'viridis', order = subgrade_order, hue_order = subgrade_order)
  sns.countplot(data = f_and_g, x = 'sub_grade', hue = 'loan_status', palette = 'viridis', order = subgrade_order)

  df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})
  df[['loan_repaid', 'loan_status']]

#Data Preprocessing
  ((df.isnull().sum())/len(df))*100

  df['emp_title'].value_counts()
  df = df.drop('emp_title', axis=1)

  sorted(df['emp_length'].dropna().unique())
  sns.countplot(data = df, x = 'emp_length', hue = 'loan_status', palette = 'Set2', order = emp_length_order)
  emp_co = df[df['loan_status'] == 'Charged Off'].groupby('emp_length').count()['loan_status']
  emp_fp = df[df['loan_status'] == 'Fully Paid'].groupby('emp_length').count()['loan_status']
  emp_len = emp_co/emp_fp
  emp_len.plot(kind = 'bar')
  df = df.drop('emp_length', axis=1)

  df['purpose'].head(10)
  df['title'].head(10)
  df = df.drop('title', axis=1)

  df.corr(numeric_only = True)['mort_acc'].sort_values()
  total_acc_avg = df.groupby('total_acc').mean(numeric_only=True)['mort_acc']
  def fill_mort_acc(total_acc, mort_acc):
      if np.isnan(mort_acc):
          return total_acc_avg[total_acc]
      else:
          return mort_acc
  df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)
  df.isnull().sum()
  df = df.dropna()

  df['term'].value_counts()
  df['term'] = df['term'].apply(lambda term: int(term[:3]))

  df = df.drop('grade',axis=1)

  dummies = pd.get_dummies(df['sub_grade'], drop_first = True)
  df = pd.concat([df.drop('sub_grade', axis =1), dummies],axis=1)

  dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose']], drop_first = True)
  df = pd.concat([df.drop(['verification_status', 'application_type','initial_list_status','purpose'], axis =1), dummies],axis=1)

  df['home_ownership'].value_counts()
  df['home_ownership'] = df['home_ownership'].replace(['NONE','ANY'],'OTHER')
  dummies = pd.get_dummies(df['home_ownership'], drop_first = True)
  df = pd.concat([df.drop('home_ownership', axis =1), dummies],axis=1)

  df['zip_code'] = df['address'].apply(lambda address:address[-5:])
  dummies = pd.get_dummies(df['zip_code'], drop_first = True)
  df = pd.concat([df.drop('zip_code', axis =1), dummies],axis=1)
  df = df.drop('address', axis=1)

  df['earliest_cr_line'] = df['earliest_cr_line'].apply(lambda date: int(date[-4:]))

#Train, test, split
  from sklearn.model_selection import train_test_split
  df = df.drop('loan_status', axis =1)
  X = df.drop('loan_repaid', axis = 1)
  y = df['loan_repaid'].values
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)
  
  from sklearn.preprocessing import MinMaxScaler
  scaler = MinMaxScaler()
  X_train = scaler.fit_transform(X_train)
  X_test = scaler.transform(X_test)

#Create the model
  from tensorflow.keras.models import Sequential
  from tensorflow.keras.layers import Dense,Dropout
  X_train.shape

  model = Sequential()
  model.add(Dense(78, activation = 'relu'))
  model.add(Dropout(0.2))
  model.add(Dense(39, activation = 'relu'))
  model.add(Dropout(0.2))
  model.add(Dense(19, activation = 'relu'))
  model.add(Dropout(0.2))
  model.add(Dense(1, activation = 'sigmoid'))
  model.compile(loss = 'binary_crossentropy', optimizer = 'adam')

  model.fit(x = X_train, y = y_train, epochs = 50, batch_size = 256, validation_data = (X_test, y_test))

#Evaluate the model
  losses = pd.DataFrame(model.history.history)
  losses.plot()
  from sklearn.metrics import classification_report, confusion_matrix
  predictions = (model.predict(X_test) > 0.5).astype("int32")
  print(classification_report(y_test, predictions))
  df['loan_repaid'].value_counts()
  317696/len(df)

#Predict a new customer
  import random
  random.seed(101)
  random_ind = random.randint(0,len(df))
  new_customer = df.drop('loan_repaid',axis=1).iloc[random_ind]
  new_customer

  new_customer = scaler.transform(new_customer.values.reshape(1,78))
  (model.predict(new_customer) > 0.5).astype("int32")
  df.iloc[random_ind]['loan_repaid']




------ AWS PROJECT ------ 


